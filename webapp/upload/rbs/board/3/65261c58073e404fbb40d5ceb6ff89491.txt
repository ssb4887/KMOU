

 - CNN(Convolution Neural Network) 관련 문제(정의 및 풀링 특징 등)
 - 전이학습(transfer learning) 관련 문제, 사전 훈련된 모델을 사용하는 방법 등
 - 순환 신경망(recurrent neural network) 및 LSTM의 특징
 - 워드 임베딩(word embedding), 생성적 절대 신경망(generative adversarial network), 오토인코더(auto encoder)
 - 강화학습 및 주요 구성요소 



1. 

풀링 : 입력데이터의 크기를 줄이는 것이다.
풀링의 장점
- 레이어의 크기가 작아지므로 계산이 빨라진다.
- 레이어의 크키가 작아진다는것은 신경망의 매개변수가 작아진다는 것을 의미한다. => 즉, 과적합이 나올 가능성이 줄어든다.
- 공간에서 물체의 이동이 있어도 결과는 변하지 않는다. 즉 물체의 공간이동에 대하여 둔감해지게 된다.

풀링의 종류 
- 최대풀링
- 평균풀링

컨볼루션(Convolution Neural Network: 컨벌루션 신경망) 신경망에서는 하위 레이어의 유닛들과 상위 레이어의 유닛들이 부분적으로만 연결되어 있다. 
따라서 복잡도가 낮아지고 과대 적합에 빠지지 않는다
컨벌루션 신경망은 모든 신경망 구조 중에서 가장 강력한 성능을 보여주는 신경망 중의 하나이다. 
컨벌루션 신경망은 2차원 형태의 입력을 처리하기 때문에, 이미지 처리에 특히 적합하다. 신경망의 각 레이어에서 일련의 필터가 이미지에 적용된다.


2. 전이 학습(transfer learning)은 하나의 문제에 대해 학습한 신경망의 모델과 가중치를, 새로운 문제에 적용하는 것이다
=>사전 학습된 신경망 모델을 다른 용도로 사용하는 것이다.

데이터 증대(data augmentation)는 한정된 데이터에서 여러 가지로 변형된 데이터를 만들어내는 기법이다.

9. Sequence data
- 일정 기간의 시간 함수 또는 순서로 표현 가능한 데이터


10. 순환 신경망
- 은닝층의 출력이 다시 은닉층의 입력으로 사용(단기 기억으로 사용)
- 단기 기억은 신경망의 은닉 가중치에 저장


RNN 유형
-One to One : 가장 기본적인 순환신경망(RNN)형태이다.
-One to Many : 이미지를 입력으로 넣고 문장을 출력하는 이미지 캡션에 활용된다.
-Many to One : 문장을 입력으로 넣고 긍정 또는 부정으로 출력하는 감성 분류에 활용된다.
-Many to Many : 기계 번역, 챗봇, 품사 예측 등 다양하게 활용되고 있다.


SimpleRNN : 단일 순환 신경망(One to One)
- 입력, 은닉상태, 출력
- 장기 기억 의존성 문제(long term dependency problem)발생
   ㄴ 멀리 떨어진 정보의 관련성을 파악하기 어려움

RNN의 한계
- RNN은 그레디언트 소실 문제로 장기 기억에 성능이 낮음

LSTM(Long Short-Term Memory)
- 장단기 메모리를 활요하여 이전 정보의 유지 여부를 결정
- LSTM은 RNN의 그래디언트 소실 문제를 개선하여 장기 기억 성능을 높임
- RNN에 비해 긴 시계열 데이터를 분류하고, 처리하거나 예측하는데 적합
- 입력/망각/출력 게이트로 구성
- 셀 상태 추가(c) => 셀 상태를 이용하여 그래디언트 전파 가능

망각 게이트
- 현재의 입력과 이전 은닉상태 값이 시그모이드 함수를 거쳐 출력됨

셀 상태 게이트
- 망각 게이트를 통해 입력되는 값을 곱해서 장기 기억이 저장되는 C 업데이트


자연어 처리 응용 분야 
- 감정 분석, 챗봇 시스템, 음성 인식, 정보 검색, 내용 요약 등

신경망 사용을 위해 단어를 수치값으로 변환 해야 할 때 사용하는 방법
정수 인코딩, 원-핫 인코딩, 워드 임배딩

정수 인코딩 : 단어를 빈도순으로 정렬한 후에, 빈도가 높은 단어부터, 번호를 차례대로 부여

원-핫 인코딩 : 이진 벡터 중에서 하나만 1이고 나머지는 모두 0  ex) cat  - 0010 0000 0000
단점 
- 벡터의 대부분이 0이기 때문에 메모리 비효율성이 높음
- 각 벡터들은 단어들 간의 유사도를 표현하지 못함

워드 임베딩 
- 단어를 밀집 벡터(dense vertor)의 형태로 표현하는 방법을 임베딩 벡터(embedding vertor)라고도 함
- 신경망 학습을 통하여 밀집 벡터를 자동으로 생성, 임베딩을 위해 LSA, Word2Vec, FastText, Glove 등의 방법론을 사용

Word2Vec : 단어 간 유사성을 고려하기 위해 단어의 의미를 벡터화 함 
- CBOW(Continuous bag of words) 모델 : 주변 단어를 사용하여 중앙의 단어 예측
- Skip-Gram 모델 : 중앙의 단어를 사용하여 주변 단어 예측

강화 학습 프레임워크
- 에이전트 : 강화 학습의 중심이 되는 객체
- 환경 : 에이전트가 작동하는 물리적 세계
- 상태 : 에이전트의 현재 상황, 미로에서의 에이전트의 위치가 상태임
- 액션 : 에이전트의 행동
- 보상 : 환경으로부터의 피드백

생성 모델
- 훈련 데이터의 규칙성 또는 패턴을 자동으로 발견하고 학습
- 훈련 데이터의 확률 분포와 유사하지만 새로운 샘플을 생성하는 신경망

인코딩 : 정보를 정해준 규칙에 따라 변환하는것  
코드화, 부호화 <- -> 디코딩, 복호화

오토인코더
- 입력과 동일한 출력을 만드는 것을 목적으로 하는 신경망
- 특징 학습, 차원 축소, 표현 학습 등에 주로 사용 됨

오토인코더 구성 요소
- 인코더(encoder) : 입력을 잠재 표현으로 인코딩
- 디코더(decoder) : 잠재 표현을 풀어서 입력을 복원
- 손실 함수 : 입력 이미지와 출력 이미지의 MSE를 사용

De-noising autoencoder : 노이즈가 있는 이미지에서 노이즈를 제거하는 용도로도 사용 가능

GAN(Generative Adversarial network) : 생성적 적대 신경망. 생성자 신경망과 판별자 신경망이 서로 적대적으로 경쟁하면서, 훈련을 통하여 자신의 작업을 점점 정교하게 수행

GAN의 구조 => 서로 대립하는 두개의 신경망으로 구성
생성자(generator) 
- 가짜 데이터를 생성하는 것을 학습
- 생성된 데이터는 판별자를 위한 데이터로 사용
판별자(discriminator)
- 생성자의 가짜 데이터를 진짜 데이터와 구분하는 방법을 학습
- 판별자는 생성자가 유사하지 않은 데이터를 생성하면 불이익을 줌




